They are theoretically intriguing because of the locality and Hebbian nature of their training algorithm (being trained by Hebb's rule), and because of their parallelism and the resemblance of their dynamics to simple physical processes. Boltzmann machines with unconstrained connectivity have not proven useful for practical problems in machine learning or inference, but if the connectivity is properly constrained, the learning can be made efficient enough to be useful for practical problems.[4]  They are named after the Boltzmann distribution in statistical mechanics, which is used in their sampling function. This is the reason why they are called "energy based models" (EBM). They were heavily popularized and promoted by Geoffrey Hinton and Terry Sejnowski in cognitive sciences communities and in machine learning.[5]